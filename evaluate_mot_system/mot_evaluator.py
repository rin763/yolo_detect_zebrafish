# ä»¥ä¸‹ã¯mot_evaluator.pyã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
"""
MOT (Multiple Object Tracking) è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - ä¿®æ­£ç‰ˆ
IDF1è¨ˆç®—ã®ãƒã‚°ã‚’ä¿®æ­£ã—ã€ã‚ˆã‚Šæ­£ç¢ºãªè©•ä¾¡ã‚’å®Ÿç¾
"""

import numpy as np
from collections import defaultdict
import json

class MOTEvaluator:
    """
    Multiple Object Tracking (MOT) è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†
    """
    
    def __init__(self, iou_threshold=0.5, distance_threshold=50):
        """
        Args:
            iou_threshold: IoUãƒãƒƒãƒãƒ³ã‚°ã®é–¾å€¤
            distance_threshold: è·é›¢ãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°ã®é–¾å€¤ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
        """
        self.iou_threshold = iou_threshold
        self.distance_threshold = distance_threshold
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ˆç®—ç”¨ï¼‰
        self.frame_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'id_switches': 0,
            'fragmentations': 0,
            'total_distance_error': 0.0,
            'matched_count': 0,
        }
        
        # IDç®¡ç†ç”¨
        self.prev_frame_matches = {}  # {gt_id: pred_id}
        self.track_status = {}  # {gt_id: {'active': bool, 'last_matched_frame': int}}
        
        # ===== ä¿®æ­£: IDF1è¨ˆç®—ç”¨ã®è©³ç´°ãªãƒãƒƒãƒãƒ³ã‚°å±¥æ­´ =====
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã®IDå¯¾å¿œã‚’è¨˜éŒ²
        self.id_matches_per_frame = []  # [(gt_id, pred_id), ...]
        
        # å„äºˆæ¸¬IDãŒã©ã®GT IDã«ä½•å›ãƒãƒƒãƒã—ãŸã‹
        self.pred_to_gt_counts = defaultdict(lambda: defaultdict(int))  # {pred_id: {gt_id: count}}
        
        # ãƒˆãƒ©ãƒƒã‚¯ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ
        self.track_lengths = defaultdict(int)
        self.track_gt_matches = defaultdict(set)
        # ===================================================
        
        # ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·
        self.current_frame = 0
        
        # è»½é‡ãªä½ç½®å±¥æ­´ï¼ˆæœ€æ–°Nå€‹ã®ã¿ä¿æŒï¼‰
        self.position_history_size = 100
        self.recent_positions = defaultdict(lambda: defaultdict(list))
        
    def compute_iou(self, box1, box2):
        """IoU (Intersection over Union) ã‚’è¨ˆç®—"""
        x1_min = box1[0] - box1[2] / 2
        y1_min = box1[1] - box1[3] / 2
        x1_max = box1[0] + box1[2] / 2
        y1_max = box1[1] + box1[3] / 2
        
        x2_min = box2[0] - box2[2] / 2
        y2_min = box2[1] - box2[3] / 2
        x2_max = box2[0] + box2[2] / 2
        y2_max = box2[1] + box2[3] / 2
        
        inter_xmin = max(x1_min, x2_min)
        inter_ymin = max(y1_min, y2_min)
        inter_xmax = min(x1_max, x2_max)
        inter_ymax = min(y1_max, y2_max)
        
        inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)
        
        box1_area = box1[2] * box1[3]
        box2_area = box2[2] * box2[3]
        union_area = box1_area + box2_area - inter_area
        
        if union_area == 0:
            return 0.0
        
        return inter_area / union_area
    
    def compute_center_distance(self, box1, box2):
        """ä¸­å¿ƒç‚¹é–“ã®è·é›¢ã‚’è¨ˆç®—"""
        return np.sqrt((box1[0] - box2[0])**2 + (box1[1] - box2[1])**2)
    
    def match_detections(self, ground_truth, predictions):
        """Ground Truthã¨Predictionã‚’ãƒãƒƒãƒãƒ³ã‚°"""
        if not ground_truth or not predictions:
            return {}, set(ground_truth.keys()), set(predictions.keys())
        
        gt_ids = list(ground_truth.keys())
        pred_ids = list(predictions.keys())
        
        cost_matrix = np.zeros((len(gt_ids), len(pred_ids)))
        
        for i, gt_id in enumerate(gt_ids):
            for j, pred_id in enumerate(pred_ids):
                iou = self.compute_iou(ground_truth[gt_id], predictions[pred_id])
                if iou >= self.iou_threshold:
                    cost_matrix[i, j] = 1 - iou
                else:
                    cost_matrix[i, j] = np.inf
        
        # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ãƒãƒƒãƒãƒ³ã‚°
        matches = {}
        unmatched_gt = set(gt_ids)
        unmatched_pred = set(pred_ids)
        
        while True:
            if np.all(np.isinf(cost_matrix)):
                break
            
            min_idx = np.unravel_index(np.argmin(cost_matrix), cost_matrix.shape)
            i, j = min_idx
            
            if cost_matrix[i, j] == np.inf:
                break
            
            gt_id = gt_ids[i]
            pred_id = pred_ids[j]
            
            matches[gt_id] = pred_id
            unmatched_gt.discard(gt_id)
            unmatched_pred.discard(pred_id)
            
            cost_matrix[i, :] = np.inf
            cost_matrix[:, j] = np.inf
        
        return matches, unmatched_gt, unmatched_pred
    
    def update_frame(self, ground_truth, predictions):
        """1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã®è©•ä¾¡ã‚’æ›´æ–°"""
        self.current_frame += 1
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ï¼‰
        if self.current_frame <= 3:
            print(f"\nğŸ” Debug Frame {self.current_frame}:")
            print(f"   GT objects: {len(ground_truth)} - IDs: {list(ground_truth.keys())[:5]}")
            print(f"   Predictions: {len(predictions)} - IDs: {list(predictions.keys())[:5]}")
        
        # ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
        matches, unmatched_gt, unmatched_pred = self.match_detections(
            ground_truth, predictions
        )
        
        if self.current_frame <= 3:
            print(f"   Matches: {len(matches)}")
            if matches:
                print(f"   Sample matches: {list(matches.items())[:3]}")
            print(f"   Unmatched GT: {len(unmatched_gt)}, Unmatched Pred: {len(unmatched_pred)}")
        
        # ãƒãƒƒãƒãƒ³ã‚°è­¦å‘Š
        if len(matches) == 0 and len(ground_truth) > 0 and len(predictions) > 0:
            if self.current_frame % 100 == 1:
                print(f"âš ï¸ Frame {self.current_frame}: No matches! GT={len(ground_truth)}, Pred={len(predictions)}")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
        self.frame_metrics['true_positives'] += len(matches)
        self.frame_metrics['false_positives'] += len(unmatched_pred)
        self.frame_metrics['false_negatives'] += len(unmatched_gt)
        self.frame_metrics['matched_count'] += len(matches)
        
        # è·é›¢èª¤å·®ã‚’è¨ˆç®—
        for gt_id, pred_id in matches.items():
            distance = self.compute_center_distance(
                ground_truth[gt_id], predictions[pred_id]
            )
            self.frame_metrics['total_distance_error'] += distance
        
        # ===== ä¿®æ­£: IDF1ç”¨ã®ãƒãƒƒãƒãƒ³ã‚°å±¥æ­´ã‚’è¨˜éŒ² =====
        for gt_id, pred_id in matches.items():
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’è¨˜éŒ²
            self.id_matches_per_frame.append((gt_id, pred_id))
            
            # å„äºˆæ¸¬IDãŒã©ã®GT IDã«ä½•å›ãƒãƒƒãƒã—ãŸã‹è¨˜éŒ²
            self.pred_to_gt_counts[pred_id][gt_id] += 1
        # ================================================
        
        # IDåˆ‡ã‚Šæ›¿ã‚ã‚Šã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
        for gt_id, pred_id in matches.items():
            if gt_id in self.prev_frame_matches:
                if self.prev_frame_matches[gt_id] != pred_id:
                    self.frame_metrics['id_switches'] += 1
                    if self.current_frame <= 10:  # ãƒ‡ãƒãƒƒã‚°
                        print(f"Frame {self.current_frame}: ID switch GT {gt_id}: {self.prev_frame_matches[gt_id]} -> {pred_id}")
            
            if gt_id in self.track_status:
                if not self.track_status[gt_id]['active']:
                    self.frame_metrics['fragmentations'] += 1
                    if self.current_frame <= 10:
                        print(f"Frame {self.current_frame}: Fragmentation GT {gt_id}")
            
            self.track_status[gt_id] = {
                'active': True,
                'last_matched_frame': self.current_frame
            }
            
            # ãƒˆãƒ©ãƒƒã‚¯çµ±è¨ˆã‚’æ›´æ–°
            self.track_lengths[pred_id] += 1
            self.track_gt_matches[pred_id].add(gt_id)
        
        # è¦‹é€ƒã•ã‚ŒãŸGT IDã¯ãƒˆãƒ©ãƒƒã‚¯ãŒé€”åˆ‡ã‚ŒãŸ
        for gt_id in unmatched_gt:
            if gt_id in self.track_status:
                self.track_status[gt_id]['active'] = False
        
        # æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãŸã‚ã«ãƒãƒƒãƒãƒ³ã‚°ã‚’ä¿å­˜
        self.prev_frame_matches = matches.copy()
        
        # ä½ç½®å±¥æ­´ã‚’æ›´æ–°
        self.recent_positions[self.current_frame] = {
            'gt': ground_truth.copy(),
            'pred': predictions.copy()
        }
        
        # å¤ã„ä½ç½®å±¥æ­´ã‚’å‰Šé™¤
        old_frames = [f for f in self.recent_positions.keys() 
                      if f < self.current_frame - self.position_history_size]
        for frame in old_frames:
            del self.recent_positions[frame]
    
    def calculate_mota(self):
        """MOTA (Multiple Object Tracking Accuracy) ã‚’è¨ˆç®—"""
        total_gt = (self.frame_metrics['true_positives'] + 
                   self.frame_metrics['false_negatives'])
        
        if total_gt == 0:
            return 0.0
        
        mota = 1 - (
            (self.frame_metrics['false_negatives'] + 
             self.frame_metrics['false_positives'] + 
             self.frame_metrics['id_switches']) / total_gt
        )
        
        return mota
    
    def calculate_motp(self):
        """MOTP (Multiple Object Tracking Precision) ã‚’è¨ˆç®—"""
        if self.frame_metrics['matched_count'] == 0:
            return 0.0
        
        motp = self.frame_metrics['total_distance_error'] / self.frame_metrics['matched_count']
        return motp
    
    def calculate_idf1(self):
        """
        IDF1 (ID F1 Score) ã‚’è¨ˆç®— - ä¿®æ­£ç‰ˆ
        æ¨™æº–çš„ãªMOTChallengeè©•ä¾¡æ–¹å¼ã«æº–æ‹ 
        """
        print(f"\nğŸ“Š IDF1 Calculation (Fixed Version):")
        
        # ===== ä¿®æ­£: æ­£ç¢ºãªIDTPã€IDFPã€IDFNã®è¨ˆç®— =====
        total_idtp = 0  # ID True Positives
        total_idfp = 0  # ID False Positives
        total_idfn = 0  # ID False Negatives
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if len(self.pred_to_gt_counts) == 0:
            print("   âš ï¸ No prediction-to-GT mappings found!")
            print("   This means no matches were recorded during tracking.")
            return 0.0
        
        print(f"   Total prediction tracks: {len(self.pred_to_gt_counts)}")
        print(f"   Total frame-level matches: {len(self.id_matches_per_frame)}")
        
        # å„äºˆæ¸¬ãƒˆãƒ©ãƒƒã‚¯ã«ã¤ã„ã¦ã€æœ€ã‚‚å¤šããƒãƒƒãƒã—ãŸGT IDã‚’ç‰¹å®š
        for pred_id, gt_counts in self.pred_to_gt_counts.items():
            if not gt_counts:
                continue
            
            # ã“ã®Pred IDãŒæœ€ã‚‚å¤šããƒãƒƒãƒã—ãŸGT ID
            most_matched_gt = max(gt_counts.items(), key=lambda x: x[1])
            best_gt_id, best_count = most_matched_gt
            
            # IDTP: æœ€ã‚‚ãƒãƒƒãƒã—ãŸGT IDã¨ã®ä¸€è‡´æ•°
            total_idtp += best_count
            
            # IDFP: ãã®ä»–ã®GT IDã¨ã®èª¤ãƒãƒƒãƒæ•°
            for gt_id, count in gt_counts.items():
                if gt_id != best_gt_id:
                    total_idfp += count
        
        # IDFN: False Negativesï¼ˆè¦‹é€ƒã—ï¼‰
        total_idfn = self.frame_metrics['false_negatives']
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        print(f"   IDTP (ID True Positives): {total_idtp}")
        print(f"   IDFP (ID False Positives): {total_idfp}")
        print(f"   IDFN (ID False Negatives): {total_idfn}")
        
        # Precision ã¨ Recall
        if (total_idtp + total_idfp) == 0:
            print(f"   âš ï¸ ID Precision = 0 (no predictions matched)")
            precision = 0.0
        else:
            precision = total_idtp / (total_idtp + total_idfp)
        
        if (total_idtp + total_idfn) == 0:
            print(f"   âš ï¸ ID Recall = 0 (no GT objects)")
            recall = 0.0
        else:
            recall = total_idtp / (total_idtp + total_idfn)
        
        print(f"   ID Precision: {precision:.4f}")
        print(f"   ID Recall: {recall:.4f}")
        
        # IDF1
        if precision + recall == 0:
            print(f"   âš ï¸ IDF1 = 0 (precision + recall = 0)")
            return 0.0
        
        idf1 = 2 * precision * recall / (precision + recall)
        print(f"   IDF1: {idf1:.4f}")
        
        return idf1
    
    def get_summary(self):
        """è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã¯ãƒªã‚»ãƒƒãƒˆã—ãªã„ï¼‰"""
        mota = self.calculate_mota()
        motp = self.calculate_motp()
        idf1 = self.calculate_idf1()
        
        summary = {
            'MOTA': mota,
            'MOTP': motp,
            'IDF1': idf1,
            'ID_Switches': self.frame_metrics['id_switches'],
            'Fragmentations': self.frame_metrics['fragmentations'],
            'True_Positives': self.frame_metrics['true_positives'],
            'False_Positives': self.frame_metrics['false_positives'],
            'False_Negatives': self.frame_metrics['false_negatives'],
            'Total_Frames': self.current_frame,
            'Avg_Track_Length': np.mean(list(self.track_lengths.values())) if self.track_lengths else 0,
        }
        
        return summary
    
    def print_summary(self):
        """è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        summary = self.get_summary()
        
        print("\n" + "="*60)
        print("MOT Evaluation Summary")
        print("="*60)
        print(f"Total Frames: {summary['Total_Frames']}")
        print(f"\nCore Metrics:")
        print(f"  MOTA (â†‘):  {summary['MOTA']:.4f}")
        print(f"  MOTP (â†“):  {summary['MOTP']:.2f} pixels")
        print(f"  IDF1 (â†‘):  {summary['IDF1']:.4f}")
        print(f"\nDetection Metrics:")
        print(f"  True Positives:  {summary['True_Positives']}")
        print(f"  False Positives: {summary['False_Positives']}")
        print(f"  False Negatives: {summary['False_Negatives']}")
        print(f"\nTracking Quality:")
        print(f"  ID Switches:     {summary['ID_Switches']}")
        print(f"  Fragmentations:  {summary['Fragmentations']}")
        print(f"  Avg Track Length: {summary['Avg_Track_Length']:.1f} frames")
        print("="*60 + "\n")
        
        return summary
    
    def save_results(self, output_path):
        """è©•ä¾¡çµæœã‚’JSONå½¢å¼ã§ä¿å­˜"""
        summary = self.get_summary()
        
        with open(output_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"Evaluation results saved to {output_path}")
    
    def reset(self):
        """è©•ä¾¡å™¨ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ–°ã—ã„è©•ä¾¡ã‚’é–‹å§‹ã™ã‚‹å ´åˆï¼‰"""
        self.frame_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'id_switches': 0,
            'fragmentations': 0,
            'total_distance_error': 0.0,
            'matched_count': 0,
        }
        self.prev_frame_matches = {}
        self.track_status = {}
        self.id_matches_per_frame = []
        self.pred_to_gt_counts = defaultdict(lambda: defaultdict(int))
        self.track_lengths = defaultdict(int)
        self.track_gt_matches = defaultdict(set)
        self.current_frame = 0
        self.recent_positions = defaultdict(lambda: defaultdict(list))
        print("ğŸ“Œ Evaluator reset completed")